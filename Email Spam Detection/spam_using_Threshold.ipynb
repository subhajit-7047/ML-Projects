{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e68e602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Naive Bayes with Threshold: 0.6 ===\n",
      "Accuracy: 0.9730941704035875\n",
      "\n",
      "Confusion Matrix:\n",
      " [[951  14]\n",
      " [ 16 134]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       965\n",
      "           1       0.91      0.89      0.90       150\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.94      0.94      0.94      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“¦ Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# âœ… Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# ðŸ§¹ Step 1: Load Data\n",
    "df = pd.read_csv('spam.csv', encoding='latin-1')[['v1', 'v2']]\n",
    "df.columns = ['label', 'message']\n",
    "\n",
    "# ðŸ§¼ Step 2: Text Cleaning\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    words = text.split()\n",
    "    filtered = [ps.stem(w) for w in words if w not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "df['cleaned'] = df['message'].apply(clean_text)\n",
    "\n",
    "# ðŸ”¢ Step 3: Label Encoding\n",
    "df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# âœ… TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['cleaned'])\n",
    "y = df['label_num']\n",
    "\n",
    "# âœ… Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# âœ… Apply SMOTE (handle class imbalance)\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# âœ… Train Naive Bayes Model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# âš ï¸ Get Prediction Probabilities\n",
    "y_proba = model.predict_proba(X_test)[:, 1]   # Probability for class \"1\" (Spam)\n",
    "\n",
    "# âœ… Threshold Tuning\n",
    "threshold = 0.60  # You can tune this value\n",
    "y_pred_thresh = (y_proba > threshold).astype(int)\n",
    "\n",
    "# âœ… Evaluation\n",
    "print(f\"=== Naive Bayes with Threshold: {threshold} ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_thresh))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_thresh))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_thresh))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953108de",
   "metadata": {},
   "source": [
    "why we use Tuning Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4def6dc5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ **Why Do We Tune the Classification Threshold?**\n",
    "\n",
    "By default, most classification models (like Logistic Regression or Naive Bayes) use a **threshold of 0.5**:\n",
    "\n",
    "* If **predicted probability â‰¥ 0.5 â†’ class = 1**\n",
    "* If **predicted probability <  0.5 â†’ class = 0**\n",
    "\n",
    "But in real-world cases (like **spam detection**, **fraud detection**, or **medical diagnosis**) this default isn't always optimal.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  **Tuning the threshold helps you control model behavior**:\n",
    "\n",
    "| Goal                                                                  | Action                                               | Why                                               |\n",
    "| --------------------------------------------------------------------- | ---------------------------------------------------- | ------------------------------------------------- |\n",
    "| âœ… Reduce **False Positives** (e.g., marking important emails as spam) | **Increase** threshold (e.g., from 0.5 â†’ 0.6 or 0.7) | Be more confident before saying something is spam |\n",
    "| âœ… Reduce **False Negatives** (e.g., missing spam emails)              | **Decrease** threshold (e.g., from 0.5 â†’ 0.4)        | Detect more spam, even if it risks some mistakes  |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š Example: Spam Detection\n",
    "\n",
    "| Email Text            | Model Confidence (Spam) | Default Prediction (0.5) | If Threshold = 0.7   |\n",
    "| --------------------- | ----------------------- | ------------------------ | -------------------- |\n",
    "| \"You won a prize!\"    | 0.95                    | Spam                     | Spam                 |\n",
    "| \"Get cheap meds now\"  | 0.60                    | Spam                     | âŒ Ham (blocked spam) |\n",
    "| \"Hi, are we meeting?\" | 0.45                    | Ham                      | Ham                  |\n",
    "\n",
    "ðŸ‘‰ In the above case, setting **threshold = 0.7** helps **avoid false positives**, but it **misses some spam**.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Summary:\n",
    "\n",
    "* **Threshold tuning = More control** over modelâ€™s sensitivity.\n",
    "* Helps **balance precision vs recall** depending on your goal.\n",
    "* Very useful when **class distribution is imbalanced** (like spam detection, fraud detection).\n",
    "\n",
    "---\n",
    "\n",
    "Want me to show a **plot of precision & recall at different thresholds**? It will help you **visually choose the best one**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3993416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
